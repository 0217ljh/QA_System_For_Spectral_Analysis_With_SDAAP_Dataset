{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel('Questions with labels.xlsx')\n",
    "\n",
    "# 创建一个空的DataFrame来保存结果\n",
    "result_df = pd.DataFrame(columns=['Question', 'Question_type', 'Entity1:Object','Entity2:Spectrum'])\n",
    "\n",
    "# 定义一个列表，包含我们感兴趣的类型\n",
    "interested_types = ['Feature Processing Method', 'machine learning Method', 'Evaluated metrics']\n",
    "\n",
    "# 遍历原始DataFrame的每一行\n",
    "for index, row in df.iterrows():\n",
    "    if row[2] in interested_types:\n",
    "        # 如果第三列的值是我们感兴趣的类型之一\n",
    "        new_row = pd.DataFrame({\n",
    "            'Question': [row[1]],\n",
    "            'Question_type': [row[2]],\n",
    "            'Entity1:Object': [row[3]],\n",
    "            'Entity2:Spectrum':[row[4]]\n",
    "        })\n",
    "        result_df = pd.concat([result_df, new_row], ignore_index=True)\n",
    "\n",
    "# 保存结果到新的Excel文件\n",
    "result_df.to_excel('Answers_type2.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义映射关系\n",
    "conversion_dict = {\n",
    "    '1': ['ultravio spectroscopy(uv)'],\n",
    "    '2': ['Visible light spectroscopy(vis)'],\n",
    "    '3.1': ['near-infrared spectroscopy(nir)'],\n",
    "    '3.2': ['mid-infrared spectroscopy(mir)'],\n",
    "    '3.3': ['Fourier transform infrared spectroscopy(ftir)'],\n",
    "    '3.4': [ 'long-wave infrared spectroscopy(lir)'],\n",
    "    '4': ['Raman spectroscopy'],\n",
    "    '5': ['Laser-induced breakdown spectroscopy(LIBS)'],\n",
    "    '6': ['terahertz spectroscopy(THz)'],\n",
    "    '7': ['fluorescence spectroscopy'],\n",
    "    '8': ['hyperspectral imaging technology'],\n",
    "    '9': ['else']\n",
    "}\n",
    "\n",
    "# 读取 Excel 文件\n",
    "file_path = 'Label_ver7_updated.xlsx'  # 更改为你的文件路径\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 转换数字到相应的文字列表\n",
    "def convert_numbers_to_text(num):\n",
    "    num = str(num)\n",
    "    # 处理可能包含多个数字的情况，例如 \"3.1, 3.2\"\n",
    "    parts = num.split(',')\n",
    "    texts = []\n",
    "    for part in parts:\n",
    "        part = part.strip()  # 清除空白\n",
    "        if part in conversion_dict:\n",
    "            texts.extend(conversion_dict[part])\n",
    "        else:\n",
    "            texts.append('Unknown')\n",
    "    return ', '.join(texts)\n",
    "\n",
    "# 指定要转换的列索引\n",
    "columns_to_convert = [11]  # Python索引为0开始，所以第5列是索引4，依此类推\n",
    "\n",
    "# 应用转换函数到指定列\n",
    "for col_index in columns_to_convert:\n",
    "    df.iloc[:, col_index] = df.iloc[:, col_index].apply(convert_numbers_to_text)\n",
    "\n",
    "# 保存修改后的 DataFrame 到新的 Excel 文件\n",
    "df.to_excel('Label.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 载入Excel文件\n",
    "df = pd.read_excel('Answers_type2.xlsx')\n",
    "\n",
    "# 定义转换关系字兄\n",
    "conversion_dict = {\n",
    "    'uv, ultravio': ['ultravio spectroscopy(uv)'],\n",
    "    ('vis, visible'): ['Visible light spectroscopy(vis)'],\n",
    "    ('nir, near-infrared'): ['near-infrared spectroscopy(nir)'],\n",
    "    ('mir, mid-infrared'): ['mid-infrared spectroscopy(mir)'],\n",
    "    ('ftir, Fourier transform infrared'): ['Fourier transform infrared spectroscopy(ftir)'],\n",
    "    ('long-wave infrared, lwir, lir'): ['long-wave infrared spectroscopy(lir)'],\n",
    "    ('raman, SERS'): ['Raman spectroscopy'],\n",
    "    ('LIBS, Laser-induced breakdown, filament-induced breakdown spectroscopy'): ['Laser-induced breakdown spectroscopy(LIBS)'],\n",
    "    ('terahertz, THz'): ['terahertz spectroscopy(THz)'],\n",
    "    ('fluoresc, fluorescence, fluorometer'): ['fluorescence spectroscopy'],\n",
    "    ('remote sensing, hyperspectral imaging'): ['hyperspectral imaging technology'],\n",
    "    ('else',): ['else']\n",
    "}\n",
    "\n",
    "# 反向映射字典\n",
    "reverse_dict = {}\n",
    "for keys, value in conversion_dict.items():\n",
    "    for key in keys:\n",
    "        reverse_dict[key] = value[0]\n",
    "\n",
    "# 函数，用于转换第四列的值\n",
    "def convert_value(val):\n",
    "    # 检查是否存在对应关系\n",
    "    for key in reverse_dict:\n",
    "        if key in val.lower():\n",
    "            return reverse_dict[key]\n",
    "    return val  # 如果没有找到对应关系，返回原始值\n",
    "\n",
    "# 应用转换函数到第四列\n",
    "df['Entity2:Spectrum'] = df['Entity2:Spectrum'].apply(convert_value)\n",
    "\n",
    "# 保存修改后的数据到新的Excel文件中\n",
    "df.to_excel('Answers_type2.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 读取Excel文件\n",
    "df1 = pd.read_excel('Answers_type2.xlsx')\n",
    "df2 = pd.read_excel(\"Label.xlsx\")\n",
    "\n",
    "# 定义TF-IDF向量化器\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8)\n",
    "\n",
    "# 准备语料库\n",
    "corpus = [str(row[5]) + \" \" + str(row[11]) + \" \" + str(row[14]) + \" \" + str(row[6]).lower() for _, row in df2.iterrows()]\n",
    "\n",
    "# 计算TF-IDF矩阵\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 定义余弦相似度检索函数\n",
    "def cosine_retrieval(keywords, dataframe, top_n=1):\n",
    "    # 将关键词转换为小写\n",
    "    keywords = keywords.strip().lower()\n",
    "    \n",
    "    # 构建查询文本的TF-IDF向量\n",
    "    query_vector = tfidf_vectorizer.transform([keywords])\n",
    "    \n",
    "    # 计算查询文本与语料库中每个文档的余弦相似度\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # 获取相似度最高的top_n行的索引\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # 返回相关度最高的论文标题以及同一行第一列的值\n",
    "    return dataframe.iloc[top_indices][dataframe.columns[0]]\n",
    "\n",
    "# 遍历df1中每行的第三列值作为关键词进行检索\n",
    "results = []\n",
    "for index, row in df1.iterrows():\n",
    "    keywords = str(row[2]) + \" \" + str(row[3])  # 第三、四列的索引\n",
    "    relevant_rows = cosine_retrieval(keywords, df2)\n",
    "    results.append(relevant_rows.values)\n",
    "\n",
    "# 将结果转换为DataFrame并与df1合并\n",
    "results_df = pd.DataFrame(results, columns=['NEW'])\n",
    "df1 = pd.concat([df1, results_df], axis=1)\n",
    "\n",
    "# 保存更新后的DataFrame到新的Excel文件\n",
    "df1.to_excel('Answers_type2.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import pandas as pd  \n",
    "from langchain.vectorstores import FAISS  \n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings  \n",
    "from langchain.chains import RetrievalQA  \n",
    "from langchain.chat_models import ChatOpenAI  \n",
    "from langchain import PromptTemplate  \n",
    "  \n",
    "# 设置Chat-GPT API的密钥  \n",
    "OPENAI_KEY = 'your key'  \n",
    "  \n",
    "# 加载Chat-GPT模型  \n",
    "gpt_model = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1, max_tokens=512, api_key=OPENAI_KEY)  \n",
    "  \n",
    "# 论文文件夹路径  \n",
    "paper_folder = \"F:\\Code\\Word_detecting\\Vector_ku\\Save_Vector_Store\" \n",
    "  \n",
    "# Excel文件路径  \n",
    "excel_path = 'Answers_type2.xlsx'  # 替换为您的Excel文件路径  \n",
    "\n",
    "# 加载Excel文件  \n",
    "df = pd.read_excel(excel_path)  \n",
    "  \n",
    "# 用于保存答案的列表  \n",
    "answers = []  \n",
    "  \n",
    "# Prompt模板  \n",
    "prompt_template = \"\"\"Based on the information of the faiss files of the related papers, briefly extract information from the text that is relevant to the question being asked.  \n",
    "  \n",
    "            known information:  \n",
    "            {context}  \n",
    "  \n",
    "            Question:  \n",
    "            {question}\"\"\"  \n",
    "  \n",
    "# 遍历Excel文件的每一行  \n",
    "for index, row in df.iterrows():  \n",
    "    paper_title = str(row.iloc[5])  # 假设第六列是论文标题  \n",
    "    query_question = row.iloc[0]  # 假设第一列是查询问题  \n",
    "      \n",
    "    # 在文件夹中查找对应的.faiss文件\n",
    "    vs_path = os.path.join(paper_folder, paper_title, 'Vector')\n",
    "    faiss_path = [f for f in os.listdir(vs_path) if f.endswith('.faiss')]\n",
    "\n",
    "    if faiss_path:  \n",
    "        # 加载向量存储  \n",
    "        vector_store = FAISS.load_local(vs_path,\n",
    "                                    HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\"),\n",
    "                                    index_name=paper_title,\n",
    "                                    allow_dangerous_deserialization=True)\n",
    "          \n",
    "        # 创建RetrievalQA链  \n",
    "        knowledge_chain = RetrievalQA.from_llm(llm=gpt_model,\n",
    "                                           retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}),\n",
    "                                           prompt=PromptTemplate(\n",
    "                                               template=prompt_template,\n",
    "                                               input_variables=[\"context\", \"question\"]))\n",
    "        result = knowledge_chain({\"query\": query_question})\n",
    "        generated_question = result['result']\n",
    "        print(paper_title,generated_question)\n",
    "        answers.append(generated_question)  \n",
    "    else:  \n",
    "        # 如果找不到.faiss文件，添加一个占位符或错误信息  \n",
    "        answers.append(\"No FAISS file found for this paper.\")  \n",
    "  \n",
    "# 将答案添加到DataFrame中  \n",
    "df['Answer'] = answers  \n",
    "  \n",
    "# 保存修改后的DataFrame回Excel文件  \n",
    "df.to_excel(excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 加载Excel文件\n",
    "file_path = 'Answers_type2.xlsx'  # 请根据实际情况修改文件路径\n",
    "sheet_name = 'Sheet1'  # 根据实际情况修改工作表名称\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "# 准备JSON数据\n",
    "json_data = []\n",
    "for index, row in df.iterrows():\n",
    "    question = row['Question']\n",
    "    question_type = row['Question_type']  # 假设第二列是问题类型\n",
    "    entity1 = row['Entity1:Object']  # 假设第三列是实体对象\n",
    "    entity2=row['Entity2:Spectrum']\n",
    "    Knowledge=row['Knowledge']\n",
    "    answer = row['Answer']\n",
    "\n",
    "    # 构造单个JSON对象\n",
    "    json_object = {\n",
    "        \"Instruction\":'Based on the information of Knowledge in this section, answer the question concisely and professionally. The content of the answer must be derived from the knowledge in this section, not randomly. The content in the answer must be the content in knowledge.',\n",
    "        \"Question\": question,\n",
    "        \"Question_type\": question_type,\n",
    "        \"Entity1:Object\": entity1,\n",
    "        \"Entity2:Spectrum\":entity2,\n",
    "        \"Knowledge\":Knowledge,\n",
    "        \"Output\": answer\n",
    "    }\n",
    "    json_data.append(json_object)\n",
    "\n",
    "# 将数据输出为JSON格式\n",
    "json_output = json.dumps(json_data, indent=4)\n",
    "print(json_output)\n",
    "\n",
    "# 如果需要，可以将JSON数据保存到文件中\n",
    "with open('Answers_type2.json', 'w') as json_file:\n",
    "    json_file.write(json_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
